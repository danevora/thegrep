**thegrep**(Daniel Evora and Peter Morrow)

**Design**: thegrep(Tar Heel egrep) is based off the class grep pattern-matching search tool created by Ken Thompson. Currently, our implementation of this classic program is split into four files: main, tokenizer, parser and nfa. The Tokenizer is responsible for taking a regular expression from the command line and turning it into meaningful tokens to be used by the Parser. The Parser then parses these tokens and creates an Abstract Syntax Tree (AST) to be used in a future part of the program. Finally, nfa takes this AST and creates an NFA (nondeterministic finite automata) which is used to determine whether inputs are accepted by a given regular expression.

**Contributions**: The work for this project was split evenly. Daniel set up the support for help flag flag in main and established the tokenizer file. Peter aided in the creation of the Tokenizer and also established the Parser, which Daniel helped complete. Both members created unit tests to test the functionality of both the Tokenizer and Parser and their accompanying helper methods. Lastly, Daniel and Peter worked together in creating the nfa.rs file, which creates fragments to construct NFA's, and uses a recursive "accepts" method that tests whether inputs are accepted by a certain regex. nfa.rs also includes methods that generate random accepted inputs when given a regex, and also an operator-overload implementation for NFA's so they may be added together. Collobration was done in person with both members alternating between who was coding and who was providing ideas of what to implement.

